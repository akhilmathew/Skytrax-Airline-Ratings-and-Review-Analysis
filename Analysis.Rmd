---
title: "Skytrax Airline Ratings and Reviews Analysis"
author: "Akhil Mathew"
date: "March 9, 2016"
output: pdf_document
---

#Introduction

This is an exploratory analysis on the airline ratings and reviews. Source of the data is <http://www.airlinequality.com/>. Airlines are segmented based on the ratings. Reviews are used to find out the reasons why certain airlines are under-rated. Various clustering techniques are used to segment the airlines. Topic modelling and Latent Dirichlet Allocation are used the text reviews.

#Data Preparation

Loading Packages

```{r,warning=FALSE}
suppressPackageStartupMessages({
require(stats)
library(dbscan)
library(plyr)
library(arm)
library(reshape2)
library(fpc)
library(dplyr)
library(Amelia)
library(cluster)
library(tm)
})
```

Read Data

```{r}
airline <- read.csv("airline.csv", header = TRUE, na.strings = c("","NA"))
seat <- read.csv("seat.csv", header = TRUE)
```

Data Wrangling

```{r}
airline <- tbl_df(airline)
airlines <- table(airline$airline_name)
aairlines <- data.frame(airlines)
colnames(aairlines) <- c("airline_name","repeats")
merge <- left_join(airline,aairlines,by="airline_name")

responses <- airline %>%
  group_by(airline_name) %>%
  summarise(response = length(which(overall_rating!="NA")))

merged <- left_join(merge,responses,by="airline_name")
merged_airline <- merged %>%
  mutate(percentage = response/repeats)
aairline <- merged_airline %>%
  filter(percentage >= 0.9,repeats >= 100)
airline_rating <- aairline[,c(1,12:16,19)]
```

Imputing Missing Values using EM Algorithm

```{r,warning==FALSE}
aairline_imputed <- amelia(airline_rating[,c(2:7)])
aairline_imp <- aairline_imputed$imputations[[1]]
aairline_rating <- bind_cols(aairline[,c(1)],aairline_imp)
```

Normalization

```{r}
mmnormalize <- function(a){
  m <- max(a,na.rm = TRUE)
  n <- min(a,na.rm = TRUE)
  mmnormalized <- a
  mmnormalized <- (mmnormalized-n)/(m-n)
  return (mmnormalized)
}

aairline_normalized <- mmnormalize(aairline_rating[,c(2:7)])
airline_proc <- bind_cols(aairline_rating[,c(1)],aairline_normalized)
airline_proc <- airline_proc[complete.cases(airline_proc),]

airline_data <- airline_proc %>%
  group_by(airline_name) %>%
  summarise_each(funs(mean))
```

#K-means Clustering

SSE Curve - Elbow curve gives us the idea about the number of clusters in the data.

```{r}
wss <- (nrow(airline_data)-1)*sum(apply(airline_data[,c(2:7)],2,var))
for (i in 2:15) wss[i] <- sum(kmeans(airline_data[,c(2:7)], 
                                     centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

km <- kmeans(airline_data[,c(2:7)],4)
clusplot(airline_data[,c(2:7)],
         km$cluster,
         color = TRUE,
         shade = TRUE,
         lines = 47,
         main = "k-means-Cluster")
```

#Principal Component Analysis

```{r}
# =======
# Run PCA
# =======

pcaresults <- prcomp(airline_data[,c(2:7)], center = TRUE, scale. = TRUE) 
#print(pcaresults)
summary(pcaresults)

pcadata <- as.data.frame(pcaresults$x)
#print(pcadata)
#summary(pcadata)

# =============
# Visualize PCA
# =============

# Show variances captured by principal components
# Type = "b" (stands for "both") shows both lines and points
plot(pcaresults$sdev, type="b", 
     main = "Variance captured by each principal component")

# Main idea: Use main two principal components 
#   to visualize multi-dimensional data in one plot

# Use biplot functionality
labels <- rep(".", nrow(airline_data))
biplot(pcaresults, cex=1.2, xlabs=labels)
# Alternative labels:
labels <- 1:nrow(airline_data)
biplot(pcaresults, cex=0.7, xlabs=labels)

# Use ggplot2 for more advanced graphs
require(ggplot2)
ggplot(data = pcadata, aes(x = PC1, y = PC2, label = "+")) +
  geom_hline(yintercept = 0, colour = "gray50") +
  geom_vline(xintercept = 0, colour = "gray50") +
  geom_text(colour = "red", size = 5) +
  ggtitle("PCA plot of Airline data")

dbscandata <- pcadata[,c(1:2)]
```

#DBSCAN Clustering

```{r}
kNNdistplot(dbscandata, k = 4)
abline(h=1.3, col="red")

#run DBScan
db <- dbscan(dbscandata, eps=1.5, MinPts=2)
plot(dbscandata, col=db$cluster)
#play around with eps

#examine cluster1
#dbscandata[db$cluster==1,]

d <- dist(dbscandata)
#metrics
cluster.stats(d, db$cluster)
plot(silhouette(db$cluster, d))
image(as.matrix(d))
```

#Hierarchical Clustering

```{r}
names <- as.vector(airline_data$airline_name)
distance <- dist(airline_data[,c(2:7)], method = "euclidean")
hcluster <- hclust(distance,method = "ward.D")
plot(hcluster,labels =names, main = "Cluster Dendrogram- Airlines")
rect.hclust(hcluster,k=4,border = "blue")

```

#Topic Modelling

##Preparing Data for Reviews Analysis

```{r}
groups <- cutree(hcluster, k=6)
clustered_data <- cbind(airline_data[,c(1:7)], groups)
clustered <- left_join(clustered_data,aairline, by="airline_name")

g1 <- subset(clustered, groups == 1)
g2 <- subset(clustered, groups == 2)
g3 <- subset(clustered, groups == 3)
g4 <- subset(clustered, groups == 4)
g5 <- subset(clustered, groups == 5)
g6 <- subset(clustered, groups == 6)
g7 <- subset(g5, g5$airline_name == "air-canada-rouge" & g5$recommended == 0)
g8 <- subset(g5, g5$airline_name == "sunwing-airlines" & g5$recommended == 0)
g9 <- subset(g5, g5$airline_name == "spirit-airlines" & g5$recommended == 0)
g10 <- subset(g5, g5$airline_name == "american-airlines" & g5$recommended == 0)
g11 <- subset(g5, g5$airline_name == "united-airlines" & g5$recommended == 0)
```



```{r,echo=FALSE}
# CLEAN DATA SET
#airline$aircraft[airline$aircraft == "319"] <- "A319"
# save 4 comments in one folder
write.table(g1$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm/comment1.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g2$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm/comment2.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g3$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm/comment3.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g4$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm/comment4.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)

# save 4 comments in seperate folder
write.table(g1$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm1/comment1.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g2$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm1/comment2.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g3$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm1/comment3.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g4$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/tm1/comment4.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)

# save 5 airlines in one folder
write.table(g7$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm/comment1.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g8$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm/comment2.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g9$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm/comment3.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g10$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm/comment4.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g11$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm/comment5.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)

# save 5 airlines in seperate folder
write.table(g7$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm1/comment1.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g8$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm1/comment2.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g9$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm1/comment3.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g10$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm1/comment4.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)
write.table(g11$content, "C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New/atm1/comment5.txt", quote = FALSE, col.names = FALSE, row.names = FALSE)

dirname <- file.path("C:/MSBA/Fall/Exploratory Data Analytics and Visualization/Exploratory Analytics and Visualization/Project/New", "atm")
```

#Data Pre-processing for Topic Modelling

```{r}
comment <- Corpus(DirSource(dirname, encoding = "UTF-8"))

meta(comment[[1]])

# The following steps pre-process the raw text documents.
# Remove punctuations and numbers because they are generally uninformative.
comment <- tm_map(comment, removePunctuation)
comment <- tm_map(comment, removeNumbers)
# Convert all words to lowercase.
comment <- tm_map(comment, content_transformer(tolower))
# Remove stopwords such as "a", "the", etc.
comment <- tm_map(comment, removeWords, stopwords("english"))

comment <- tm_map(comment, removeWords, c("flight","airline","plane")) 
# Use the SnowballC package to do stemming.
library(SnowballC)
comment <- tm_map(comment, stemDocument)
# Remove excess white spaces between words.
comment <- tm_map(comment, stripWhitespace)
# Inspect the first document to see what it looks like.
#comment[["comment1.txt"]]$content
# Convert all documents to a term frequency matrix.
tfm <- DocumentTermMatrix(comment)
# We can check the dimension of this matrix by calling dim()
#print(dim(tfm))
```

#Initial Analysis

```{r}
dtm <- TermDocumentMatrix(comment)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#head(d, 10)

#findAssocs only works when there's more than one document
findAssocs(dtm, terms = "uncomfortable", corlimit = 0.95)

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most Frequent Words",
        ylab = "Word Frequencies")
```

#Topic Modelling

```{r}
# Use topicmodels package to conduct LDA analysis.
library(topicmodels)
results <- LDA(tfm, k = 10, method = "Gibbs")
# Obtain the top five words (i.e., the 5 most probable words) for each topic.
Terms <- terms(results, 20)
# Obtain the most likely topic assignment for each document.
Topic <- topics(results, 1)
# Get the posterior probability for each document over each topic
posterior <- posterior(results)[[2]]

#look at the posterior topic distribution for the first document and plot it visually
#posterior(results)[[2]][1,]
barplot(posterior(results)[[2]][1,],main = "Topic Distribution")

# Calculate the entropy for each document to quantify keyword ambiguity
CalcEntropy <- function(document) {
  entropy = 0
  for (i in 1:length(document)) {
    entropy = entropy - document[i]*log(document[i])
  }
  return(entropy)
}

Entropy <- apply(posterior, 1, CalcEntropy) #posterior is matrix, 1 indicates rows
newKeywordConstruct <- data.frame(Entropy, Topic)
```


#Reflections

It was a deep exploration. We have found the segments in the airlines, especially the poorly rated ones. Focusing our analysis onto those airlines pointed us to a few reasons why they are under rated. Recommendations are documented according to these findings.Please find it in the presentation included.